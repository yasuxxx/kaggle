{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn           # 神经网络工具箱torch.nn\n",
    "import torch.nn.functional as F # 神经网络函数torch.nn.functional\n",
    "import torch.utils.data as tud  # Pytorch读取训练集需要用到torch.utils.data\n",
    "\n",
    "from torch.nn.parameter import Parameter # 参数更新和优化函数\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import scipy          # SciPy是基于Numpy开发的高级模块，它提供了许多数学算法和函数的实现\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity # 余弦相似度函数\n",
    "USE_CUDA = torch.cuda.is_available()  # 有GPU可以用,没有用CPU\n",
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# 设定一些超参数\n",
    "K = 100  # number of negative samples 负样本随机采样数量\n",
    "C = 3  # nearby words threshold 指定周围三个单词进行预测\n",
    "NUM_EPOCHS = 2  # The number of epochs of training 迭代轮数，default=10\n",
    "MAX_VOCAB_SIZE = 30000  # the vocabulary size 词汇表多大\n",
    "BATCH_SIZE = 32  # the batch size 每轮迭代1个batch的数量\n",
    "LEARNING_RATE = 0.2  # the initial learning rate #学习率\n",
    "EMBEDDING_SIZE = 100  # 词向量维度\n",
    "\n",
    "\n",
    "LOG_FILE = \"word_embedding.log\"\n",
    "\n",
    "# tokenize函数，把一篇文本转化成一个个单词\n",
    "\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return text.split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: PREFACE\n",
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrib\n"
     ]
    },
    {
     "data": {
      "text/plain": "17683"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/nietzsche.txt\",\"r\") as f:\n",
    "    text =f.read()\n",
    "\n",
    "print(f'text: {text[:200]}')\n",
    "\n",
    "text = [w for w in word_tokenize(text.lower())]\n",
    "# 分词\n",
    "\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE-1))\n",
    "# 取出最常用的MAX_VOCAB_SIZE个单词，-1留给不常见的单词\n",
    "\n",
    "vocab[\"<unk>\"] =  len(text) - np.sum(list(vocab.values()))\n",
    "# unk表示不常见的单词数=总单词数-常见单词数\n",
    "\n",
    "idx_to_word = [word for word in vocab.keys()]\n",
    "\n",
    "word_to_idx = {word:i for i,word in enumerate(idx_to_word)}\n",
    "\n",
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "\n",
    "word_freqs = word_freqs/np.sum(word_freqs)\n",
    "\n",
    "VOCAB_SIZE = len(idx_to_word)\n",
    "VOCAB_SIZE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self,text,word_to_idx,idx_to_word,word_freqs,word_counts):\n",
    "        super(WordEmbeddingDataset,self).__init__()\n",
    "        # 将字符转换成索引\n",
    "        self.text_encoded = [word_to_idx.get(t,VOCAB_SIZE-1) for t in text]\n",
    "\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        :param idx:\n",
    "        :return:\n",
    "                - 中心词\n",
    "                - 这个单词附近的positive单词\n",
    "                - 随机采样的k个单词作为negative sample\n",
    "        '''\n",
    "\n",
    "        center_word = self.text_encoded[idx]\n",
    "        pos_indices = list(range(idx-C,idx)) + list(range(idx+1,idx+C+1))\n",
    "        # 周围词索引的索引，比如idx=0时。pos_indices = [-3, -2, -1, 1, 2, 3]\n",
    "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
    "        # range(idx+1, idx+C+1)超出词汇总数时，需要特别处理，取余数\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # 周围词的索引\n",
    "\n",
    "        neg_words = torch.multinomial(\n",
    "            self.word_freqs,K*pos_words.shape[0],True\n",
    "        )\n",
    "        # 负采样单词索引\n",
    "\n",
    "        return center_word,pos_words,neg_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[600]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = WordEmbeddingDataset(\n",
    "    text,word_to_idx,idx_to_word,word_freqs,word_counts\n",
    ")\n",
    "\n",
    "list(dataset[0][2].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x7fe11de199a0>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = tud.DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   28,   160,   331, 12247,  1783,    41,     6,    11,  1284,   657,\n",
      "            1,    76,  1584,   248,  1038,    51,     6,  7296,  1479, 17549,\n",
      "          122,   293,    31,     0,     2, 13459,   361,     1,    42,    17,\n",
      "            4,   235]) tensor([[11454,  5081,    49,    23,   151,    84],\n",
      "        [ 1340,     8,  4835,     0,  1946,     1],\n",
      "        [    4,    40,     6,    38,  2104,     8],\n",
      "        [ 2398,    57,   179,    25,  3228,  4246],\n",
      "        [    5,  4026,     0,    11,    99,     4],\n",
      "        [   77,  1041,     7,     0, 10907,     1],\n",
      "        [    4,    19,   292,  3474,     2, 14189],\n",
      "        [    0, 10307,   478,  1633, 10308,  1633],\n",
      "        [12915, 12916, 12917, 12918, 12919,  3616],\n",
      "        [ 1309,     1,     0,   549,   108,   303],\n",
      "        [15336,     0,  1600,     0,   355, 15337],\n",
      "        [ 8986,     3,  4552,     4,  8987,     2],\n",
      "        [ 4139,  4751,     7,    27,    16,   328],\n",
      "        [ 2679,  1700,  1228,  6175,     3,     7],\n",
      "        [   51,  1351,     3,     1,    17,  9750],\n",
      "        [   17,   703,   151,    43, 10753, 10754],\n",
      "        [    1, 15586,    77,    34,  5357,    33],\n",
      "        [    0,   596,     1,  7297,     2,    40],\n",
      "        [12059,   198,     1,   482,   927,     1],\n",
      "        [   99,     2,    25,  1624,   717,  2836],\n",
      "        [    5, 13767,  5435,    15,     0,  5436],\n",
      "        [   11,    20,    32,     4,     0,   145],\n",
      "        [ 1005,    22, 12907,   381,  3287,  2061],\n",
      "        [   20,   338,     4,   862,     1,    32],\n",
      "        [    6,  7185,  4048,  2855,   526,     5],\n",
      "        [  209,  3351,     1,   318,   917,    75],\n",
      "        [ 2235, 12115,     1,  1543,   224,    31],\n",
      "        [    5, 13983, 13984,  5471,     3,     7],\n",
      "        [  927,     6,   209,     5,  2738,  3297],\n",
      "        [  392,     1,   425,    92,   789,    18],\n",
      "        [    5,   123,    95,   715, 14074,  5487],\n",
      "        [  745,     3,    11,    20,  4352,     7]]) tensor([[ 1188,   287,    68,  ...,   587,  2372, 12059],\n",
      "        [14325,   574,  6904,  ...,  4325,  5319,  3164],\n",
      "        [15930,  6866, 14987,  ...,    12,   109,    39],\n",
      "        ...,\n",
      "        [  215,     1,  1081,  ...,  1934,  1103, 15158],\n",
      "        [ 7421, 13937,    63,  ...,     3, 12522, 16223],\n",
      "        [   24,  2542, 11137,  ...,     8,     9,   631]])\n",
      "tensor([   10,    12,     3,  1601,    71, 12307, 16574,   130,    63, 16327,\n",
      "            2,    29,     3,  4534,   829,  4151,   667,     0,     3,    23,\n",
      "            2,   114,  3632, 16275,  3885,    15,  9227,  1520,   576,    47,\n",
      "           67,    60]) tensor([[  848,    55,   628,   397, 16545, 16546],\n",
      "        [  976,   788,   296,   463,   635,    53],\n",
      "        [ 2600,     6,  3541,     5, 15561,     0],\n",
      "        [   18,   509,  3134,     6,  3134,    36],\n",
      "        [    3, 16188,    25,   273,     1,   106],\n",
      "        [    3,     0,  3347,    11,     0, 12308],\n",
      "        [ 2650,    50,   144,     0,  3413,   206],\n",
      "        [   19,  2349,     8,    49,     0,   206],\n",
      "        [    1, 16102,    18,   930,     4, 16103],\n",
      "        [   82,  3508,     3,     3,    11,     6],\n",
      "        [   46,   487, 10536, 10537,    10,     0],\n",
      "        [  253,    70,   427,    16,  9539,  9540],\n",
      "        [10171,   258,     2, 10172,     0,   403],\n",
      "        [ 2924,  4533,  1135,     2,  8905,    20],\n",
      "        [  337,  3697,   164,     9,     6,  1094],\n",
      "        [    2,  1623,     0,     0,  1802,     0],\n",
      "        [17518,     1,    25,   121,   829,     9],\n",
      "        [ 3260,     1,   914,   192,    39,  1623],\n",
      "        [    5,   152, 17225,    77,     0,  3492],\n",
      "        [   17, 13791,    48,    51,  5440,  5441],\n",
      "        [  112,    23,  8314,  8315,   107,  1412],\n",
      "        [   31,   815,  7546,  4155,    14,   735],\n",
      "        [  226,     4,     0,     1,  1063,     4],\n",
      "        [  704,    15,    61,    10,    61,     2],\n",
      "        [ 2163,    24,     0,    10,     5,   631],\n",
      "        [  208,   553,     7,     6,    13,     5],\n",
      "        [   49,    28,    18,     2,  1004,     3],\n",
      "        [    6,   114, 16881,     3,   695,     2],\n",
      "        [ 1021,  3040,     2, 10035,     9, 10036],\n",
      "        [ 3578,     1,  1205,   140,    27,     5],\n",
      "        [15948,     2,  1850,     0,  1357,    66],\n",
      "        [   41,  7831,  7832,  4261,     3,    77]]) tensor([[ 8733,  2188,   636,  ...,  5693,  7085,    73],\n",
      "        [   10,   650,  1147,  ...,  4956,  4974,  2646],\n",
      "        [16841,  9565,   286,  ..., 10173,   150,  3405],\n",
      "        ...,\n",
      "        [ 3365,    22,    10,  ...,  2246,  9211,   154],\n",
      "        [ 1746,    17,   480,  ...,    83, 15991,  8146],\n",
      "        [  852,  2196,   373,  ...,   639, 13647,  4891]])\n",
      "tensor([    9,     0,     7,     2,     2,     8,   130,   218,  2424,    16,\n",
      "            3,  1674,   122,    15,    30,    16,     4,  4713,   603,   102,\n",
      "           19,   120,     0,  4888,     0,     0,     1,  5692,   121,    79,\n",
      "        16254, 13852]) tensor([[    2,  1322, 14202,   522,     5, 14203],\n",
      "        [    2,     1,  7219,  1225,   412,     1],\n",
      "        [ 3548,     1,   120,     6,  2284,     4],\n",
      "        [   66,   116,  4989,  1429,     4,     0],\n",
      "        [ 1092,  1048, 12904, 12905,    55,    12],\n",
      "        [    0,   461,    96,     5,  1540,   978],\n",
      "        [    4,   193,     8,     8,     4,  8149],\n",
      "        [   40,     9,  1609,     2,  1291,   768],\n",
      "        [    2, 16323,     0,     0,  2203,     0],\n",
      "        [ 5386,    16,   534,   429,    11,    75],\n",
      "        [ 4086,     0,   728,   197,  8549,     0],\n",
      "        [    0,  2565,     2,   772,  5516,     4],\n",
      "        [  106,  1772,     1,   407,  6958,     2],\n",
      "        [   25,  3006,  4832,    52,    13,   386],\n",
      "        [    5,  3544,   766,    18,  1485,     3],\n",
      "        [   23,  3427,     3,    71,  4843,    16],\n",
      "        [  951,  6068,    41,    96,   571,  2651],\n",
      "        [ 9622,     2,  1674,     1,     0,   135],\n",
      "        [ 3148,     2,  1441,    37,   262,    12],\n",
      "        [  123,   918, 10843,    28,    23,     0],\n",
      "        [   11,  6119,  6120,     3,   258,    63],\n",
      "        [ 8469,    13,    25,     1,   842,    27],\n",
      "        [   35,     6,  3267, 11935,  1390,     0],\n",
      "        [ 1268,     5,  1626,     2,  3067,   257],\n",
      "        [    5,    69,   488,  4317,     1,  8066],\n",
      "        [    0,  6659,     1,  6660,     4,    17],\n",
      "        [    0,  1408,   947,  5948,     9,   156],\n",
      "        [  981,    83,   440,   197, 15383,  5689],\n",
      "        [    9,     6,   280, 10701,     3,  1739],\n",
      "        [    0,  1553,     1,   170,  5727,     6],\n",
      "        [  132,     5,    87,  5796,    56,     0],\n",
      "        [13851,     2,  5452, 13853, 13854,  2929]]) tensor([[  119, 13616, 13374,  ..., 15055,     0,  8238],\n",
      "        [    1,    12, 14289,  ...,  4811,   681,  1009],\n",
      "        [ 5545,  3383,   149,  ..., 14673, 10915, 11570],\n",
      "        ...,\n",
      "        [15077,   114,    30,  ...,  7676,  3266,  9093],\n",
      "        [ 5091,   252,  2050,  ..., 16974,    10,    72],\n",
      "        [   53,   562, 11161,  ...,   182,     5,  3454]])\n",
      "tensor([    1,    46,    23,    10,   734,     0,     0,    53, 16895,    18,\n",
      "        11555,     5,     0,  5640,     1,    16,     0,    70,    84,     3,\n",
      "        15926,     2,    18,   189,   168,     1,   201,    32,    42,     0,\n",
      "            1,   199]) tensor([[   10,     0,  3167, 12086,     2, 12087],\n",
      "        [   46, 10246,     0, 10247,     0,    46],\n",
      "        [  362, 17005,    80,   143,    36,  4213],\n",
      "        [    8,    53,    93,     0,   109,    95],\n",
      "        [   11,   624,     3,  2486, 17279,  2775],\n",
      "        [   11,    16,  4835,   304,  4836,     1],\n",
      "        [   27,    69,     4,   188,     1,   508],\n",
      "        [    9,     6,    41,    17,   235,    12],\n",
      "        [    3,   305,     9,   158,  2317,     6],\n",
      "        [  760,     1,   518,    42,     5,  8849],\n",
      "        [    2,    65, 11554,     4,    17,     7],\n",
      "        [ 2194,   345,     3,  2064,   136,     0],\n",
      "        [  283,     9,     6,  1024,  1051,     7],\n",
      "        [ 1531,   264,     1,     2, 16323,     0],\n",
      "        [    0,   323,   125,     0,    91,     2],\n",
      "        [ 5971,     2,  3606,  5972,    27,     0],\n",
      "        [  332,  2593,    33,    46,   365,  2031],\n",
      "        [ 3470,     1,     5, 14075,    70, 14076],\n",
      "        [  319,     5,   247,   153,  9018,   269],\n",
      "        [  145,     9,   468,     0,   327,     1],\n",
      "        [   21,   594,     3,    76,     3,  4150],\n",
      "        [   70, 12890,  1634,  5298,    26,  2405],\n",
      "        [   75,    84,     2,    70,   440,     0],\n",
      "        [  851, 15299,    22,     7,   156,     3],\n",
      "        [    1,   670,     0,     4,    25,   178],\n",
      "        [    5,  1540,   212,  7386,     0,   542],\n",
      "        [13185,     8,    21, 13186,     0,   196],\n",
      "        [    3, 12061,   281,   844,    10, 12062],\n",
      "        [    3, 15461,  3486,   102,    39,  3499],\n",
      "        [15054,    15,  4672,   155,     0,   550],\n",
      "        [  137,    84,   626,    17,     0,    91],\n",
      "        [    6, 14618,     1,    11,     6,    45]]) tensor([[    0,   118, 16864,  ...,  7732,  5551,  1574],\n",
      "        [ 2959,   128, 17589,  ...,    58, 16736,  1597],\n",
      "        [   10,  2066,  3503,  ...,   183,  1643,  3126],\n",
      "        ...,\n",
      "        [ 2020,  8469,     6,  ...,     6,  7740,   529],\n",
      "        [ 5124,    26,  8108,  ...,  8513,     3,   300],\n",
      "        [ 6284,     5,   515,  ...,   490,    70, 14568]])\n"
     ]
    }
   ],
   "source": [
    "for i,(input_labels,pos_labels,neg_labels) in enumerate(dataloader):\n",
    "    print(input_labels,pos_labels,neg_labels)\n",
    "    if i > 2:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class EmbeddinModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size):\n",
    "        super(EmbeddinModel,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.out_embed = nn.Embedding(\n",
    "            self.vocab_size, self.embed_size, sparse=False)\n",
    "        #模型输出nn.Embedding(30000, 100)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        # 权重初始化的一种方法\n",
    "\n",
    "        self.in_embed = nn.Embedding(\n",
    "            self.vocab_size, self.embed_size, sparse=False)\n",
    "        #模型输入nn.Embedding(30000, 100)\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        # 权重初始化的一种方法\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        \"\"\"\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]\n",
    "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]\n",
    "\n",
    "        return: loss, [batch_size]\n",
    "        \"\"\"\n",
    "\n",
    "        # input_labels是输入的标签，tud.DataLoader()返回的。相已经被分成batch了。\n",
    "        batch_size = input_labels.size(0)\n",
    "\n",
    "        input_embedding = self.in_embed(input_labels)\n",
    "        # B * embed_size\n",
    "        # 这里估计进行了运算：（128,30000）*（30000,100）= 128(B) * 100 (embed_size)\n",
    "\n",
    "        pos_embedding = self.out_embed(pos_labels)  # B * (2*C) * embed_size\n",
    "        # 同上，增加了维度(2*C)，表示一个batch有B组周围词单词，一组周围词有(2*C)个单词，每个单词有embed_size个维度。\n",
    "\n",
    "        # B * (2*C * K) * embed_size\n",
    "        neg_embedding = self.out_embed(neg_labels)\n",
    "        # 同上，增加了维度(2*C*K)\n",
    "\n",
    "        # torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        log_pos = torch.bmm(\n",
    "            pos_embedding, input_embedding.unsqueeze(2)).squeeze()  # B * (2*C)\n",
    "        log_neg = torch.bmm(\n",
    "            neg_embedding, -input_embedding.unsqueeze(2)).squeeze()  # B * (2*C*K)\n",
    "        # unsqueeze(2)指定位置升维，.squeeze()压缩维度。\n",
    "\n",
    "        # 下面loss计算就是论文里的公式\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1)  # batch_size\n",
    "        loss = log_pos + log_neg\n",
    "\n",
    "        return -loss\n",
    "\n",
    "    def input_embeddings(self):  # 取出self.in_embed数据参数\n",
    "        return self.in_embed.weight.data.cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model = EmbeddinModel(VOCAB_SIZE,EMBEDDING_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 420.04718017578125\n",
      "epoch: 0, iter: 100, loss: 267.3180236816406\n",
      "epoch: 0, iter: 200, loss: 189.4784393310547\n",
      "epoch: 0, iter: 300, loss: 217.42340087890625\n",
      "epoch: 0, iter: 400, loss: 145.1310272216797\n",
      "epoch: 0, iter: 500, loss: 131.80780029296875\n",
      "epoch: 0, iter: 600, loss: 157.67408752441406\n",
      "epoch: 0, iter: 700, loss: 113.30000305175781\n",
      "epoch: 0, iter: 800, loss: 179.28933715820312\n",
      "epoch: 0, iter: 900, loss: 79.97330474853516\n",
      "epoch: 0, iter: 1000, loss: 83.37088012695312\n",
      "epoch: 0, iter: 1100, loss: 124.0562515258789\n",
      "epoch: 0, iter: 1200, loss: 79.22520446777344\n",
      "epoch: 0, iter: 1300, loss: 114.33177185058594\n",
      "epoch: 0, iter: 1400, loss: 103.77493286132812\n",
      "epoch: 0, iter: 1500, loss: 76.44828033447266\n",
      "epoch: 0, iter: 1600, loss: 121.24266052246094\n",
      "epoch: 0, iter: 1700, loss: 146.60772705078125\n",
      "epoch: 0, iter: 1800, loss: 109.6523208618164\n",
      "epoch: 0, iter: 1900, loss: 36.54298782348633\n",
      "epoch: 0, iter: 2000, loss: 75.1758804321289\n",
      "epoch: 0, iter: 2100, loss: 85.2036361694336\n",
      "epoch: 0, iter: 2200, loss: 98.90494537353516\n",
      "epoch: 0, iter: 2300, loss: 94.00745391845703\n",
      "epoch: 0, iter: 2400, loss: 124.8311538696289\n",
      "epoch: 0, iter: 2500, loss: 59.34922790527344\n",
      "epoch: 0, iter: 2600, loss: 72.37234497070312\n",
      "epoch: 0, iter: 2700, loss: 85.4643325805664\n",
      "epoch: 0, iter: 2800, loss: 47.47477340698242\n",
      "epoch: 0, iter: 2900, loss: 82.67478942871094\n",
      "epoch: 0, iter: 3000, loss: 51.64985656738281\n",
      "epoch: 1, iter: 0, loss: 37.289554595947266\n",
      "epoch: 1, iter: 100, loss: 41.4534912109375\n",
      "epoch: 1, iter: 200, loss: 36.64368438720703\n",
      "epoch: 1, iter: 300, loss: 37.961143493652344\n",
      "epoch: 1, iter: 400, loss: 40.1470947265625\n",
      "epoch: 1, iter: 500, loss: 36.36509323120117\n",
      "epoch: 1, iter: 600, loss: 47.10612487792969\n",
      "epoch: 1, iter: 700, loss: 45.665096282958984\n",
      "epoch: 1, iter: 800, loss: 35.64337158203125\n",
      "epoch: 1, iter: 900, loss: 42.14067840576172\n",
      "epoch: 1, iter: 1000, loss: 34.72463607788086\n",
      "epoch: 1, iter: 1100, loss: 38.71413040161133\n",
      "epoch: 1, iter: 1200, loss: 37.372989654541016\n",
      "epoch: 1, iter: 1300, loss: 47.56955337524414\n",
      "epoch: 1, iter: 1400, loss: 38.76508331298828\n",
      "epoch: 1, iter: 1500, loss: 33.71506881713867\n",
      "epoch: 1, iter: 1600, loss: 35.86991500854492\n",
      "epoch: 1, iter: 1700, loss: 45.35934066772461\n",
      "epoch: 1, iter: 1800, loss: 35.563533782958984\n",
      "epoch: 1, iter: 1900, loss: 33.34968566894531\n",
      "epoch: 1, iter: 2000, loss: 36.55980682373047\n",
      "epoch: 1, iter: 2100, loss: 36.61572265625\n",
      "epoch: 1, iter: 2200, loss: 33.84832000732422\n",
      "epoch: 1, iter: 2300, loss: 45.9534797668457\n",
      "epoch: 1, iter: 2400, loss: 35.57599639892578\n",
      "epoch: 1, iter: 2500, loss: 37.58352279663086\n",
      "epoch: 1, iter: 2600, loss: 34.49294662475586\n",
      "epoch: 1, iter: 2700, loss: 34.38151550292969\n",
      "epoch: 1, iter: 2800, loss: 36.21903991699219\n",
      "epoch: 1, iter: 2900, loss: 35.366844177246094\n",
      "epoch: 1, iter: 3000, loss: 33.18097686767578\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "# 随机梯度下降\n",
    "\n",
    "for e in range(NUM_EPOCHS):  # 开始迭代\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        # print(input_labels, pos_labels, neg_labels)\n",
    "\n",
    "        input_labels = input_labels.long()  # longtensor\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        if USE_CUDA:\n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()\n",
    "            neg_labels = neg_labels.cuda()\n",
    "\n",
    "        # 下面第一节课都讲过的\n",
    "        optimizer.zero_grad()  # 梯度归零\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印结果。\n",
    "        if i % 100 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(\n",
    "                    e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "\n",
    "        # if i % 2000 == 0:\n",
    "        #     embedding_weights = model.input_embeddings()\n",
    "        #     sim_simlex = evaluate(\"simlex-999.txt\", embedding_weights)\n",
    "        #     sim_men = evaluate(\"men.txt\", embedding_weights)\n",
    "        #     sim_353 = evaluate(\"wordsim353.csv\", embedding_weights)\n",
    "        #     with open(LOG_FILE, \"a\") as fout:\n",
    "        #         print(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "        #             e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "        #         fout.write(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "        #             e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "\n",
    "    embedding_weights = model.input_embeddings()\n",
    "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights)\n",
    "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}