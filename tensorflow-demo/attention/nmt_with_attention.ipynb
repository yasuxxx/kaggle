{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 将 unicode 文件转换为 ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
    "    # 例如： \"he is a boy.\" => \"he is a boy .\"\n",
    "    # 参考：https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # 给句子加上开始和结束标记\n",
    "    # 以便模型知道何时开始和结束预测\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 1. 去除重音符号\n",
    "# 2. 清理句子\n",
    "# 3. 返回这样格式的单词对：[ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=''\n",
    "    )\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    # 返回的对象是ndarray（numpy的n维数组对象）\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                           padding='post')\n",
    "    return tensor, lang_tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def load_dataset(path,num_examples=None):\n",
    "    targ_lang,inp_lang = create_dataset(path,num_examples)\n",
    "\n",
    "    input_tensor,inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor,targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor,inp_lang_tokenizer,target_tensor,targ_lang_tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "input_tensor,inp_lang_tokenizer,target_tensor,targ_lang_tokenizer = load_dataset(path_to_file,num_examples)\n",
    "\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# 采用 80 - 20 的比例切分训练集和验证集\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# 显示长度\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def convert(lang,tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print(f'{t}------->{lang.index_word[t]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-------><start>\n",
      "6------->¿\n",
      "66------->puede\n",
      "13------->la\n",
      "327------->gente\n",
      "1398------->cambiar\n",
      "5------->?\n",
      "2-------><end>\n",
      "#########################################\n",
      "1-------><start>\n",
      "25------->can\n",
      "329------->people\n",
      "541------->change\n",
      "7------->?\n",
      "2-------><end>\n"
     ]
    }
   ],
   "source": [
    "convert(inp_lang_tokenizer,input_tensor_train[0])\n",
    "print('#########################################')\n",
    "convert(targ_lang_tokenizer,target_tensor_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:56:53.164898: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-13 14:56:53.317580: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2899885000 Hz\n",
      "2022-04-13 14:56:53.317931: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5568df5b6670 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-04-13 14:56:53.317943: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-04-13 14:56:53.324057: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorShape([64, 16]), TensorShape([64, 11]))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch,example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape,example_target_batch.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "encoder基本内容\n",
    "1. __init__\n",
    "2. call\n",
    "3. initilizer(参数初始化器)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_sz,embedding_dim,units,batch_sz,):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz =  batch_sz\n",
    "        self.enc_units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_sz,\n",
    "            output_dim=embedding_dim\n",
    "        )\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            units = units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "    def call(self, x, hidden_state):\n",
    "        x = self.embedding(x)\n",
    "        outputs,enc_hiddens = self.gru(x,hidden_state)\n",
    "        return outputs,enc_hiddens\n",
    "\n",
    "     # 确定初始状态的大小\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz,self.enc_units))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size,embedding_dim,units,BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output,sample_hidden = encoder(example_input_batch,sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意力机制模块：\n",
    "    inputs：values(batch_sz*seq_length*enc_units)、query(batch_sz*dec_emded_num)\n",
    "    outputs:context_vector(batch_sz*enc_units)、attention_weights(batch_sz*seq_length)\n",
    "计算步骤：\n",
    "   scores = V*tanh(W1*values+W2*query)   batch*seq_length*1\n",
    "   attention_weights = softmax(scores)   batch*seq_length*1\n",
    "   context_vector =  reduce_sum(attentions_weights*values,axis=1)  batch_sz*enc_units\n",
    "具体函数\n",
    "    1. __init__\n",
    "    2. call"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self,query,values):\n",
    "    # values: batch_sz*seq_length*dec_units\n",
    "    # query: batch_sz*dec_dim_nums\n",
    "    query = tf.expand_dims(query,1)     # batch_sz*1*dec_dim_nums\n",
    "    scores = self.V(tf.nn.tanh(self.W1(values)+self.W2(query))) # batch_sz*seq_length*1\n",
    "    attention_weights = tf.nn.softmax(scores,axis=1)  # batch_sz*seq_length*1\n",
    "    context_vector = tf.reduce_sum(attention_weights*values,axis=1)\n",
    "    # print(f'contex_tvector s shape is {context_vector.shape}')\n",
    "    return context_vector, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_hidden s shape is (64, 1024)\n",
      "sample_out s shape is (64, 16, 1024)\n",
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "print(f'sample_hidden s shape is {sample_hidden.shape}')\n",
    "print(f'sample_out s shape is {sample_output.shape}')\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "解码模块：\n",
    "    inputs:hidden_states(batch_sz*units)、inputs(batch_sz*1)、pre_output(batch_sz*units)\n",
    "    outputs:hidden_states(batch_sz*units)、outputs(batch_sz*vocab_sz)\n",
    "计算思路："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,dec_units,batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vacab_sz = vocab_size\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim\n",
    "        )\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            units=units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden_states, enc_output):\n",
    "        x = self.embedding(x)                                               # batch_sz*1*embed_dim_num\n",
    "        # print(f'x s shape is {x.shape}')\n",
    "        # print(f'enc_output s shape is {enc_output.shape}')\n",
    "        context_vector,_ = self.attention(hidden_states,enc_output)                     # batch_sz*units\n",
    "        # print(f'context_vector s shape is {context_vector.shape}')\n",
    "        x = tf.concat([x,tf.expand_dims(context_vector,1)],-1)              # batch_sz*1*(embed_dim_num+unitis)\n",
    "        output,state = self.gru(x)\n",
    "        output = tf.squeeze(output,axis=1)\n",
    "        output = self.fc(output)\n",
    "        return output,state,_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size,embedding_dim,units,BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output,_,_ = decoder(tf.random.uniform((64,1)),sample_hidden,sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义优化器和损失函数\n",
    "1. 优化器采用Adam\n",
    "2. 损失函数用sparseCategoricalCrossentropy\n",
    "3. 计算loss时需要通过mask来fill掉零值\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "def loss_function(y_true,y_pred):\n",
    "    mask = tf.not_equal(y_true,0)\n",
    "    loss_ = loss_object(y_true,y_pred)\n",
    "    mask = tf.cast(mask,dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "检查点（基于对象保存）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    optimizer=optimizer,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练：（示教模式）\n",
    "1. 将输入喂入编码器，编码器返回编码器输出和编码器隐藏层状态\n",
    "2. 编码器输出、编码器隐藏层状态和解码器输入喂如解码器，解码器返回预测和解码器隐藏层状态\n",
    "3. 预测用于计算损失，解码器隐藏层状态传送回模型\n",
    "4. 使用teacher forcing决定解码器的下一个输入（将目标词作为下一个输入）\n",
    "5. 计算梯度，应用于优化器和反向传播"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def train_step(inp, targ ,enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output,enc_hidden = encoder(inp,enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']]*BATCH_SIZE,1)\n",
    "        dec_input = tf.cast(dec_input,dec_hidden.dtype)\n",
    "        for t in range(1,targ.shape[1]):\n",
    "            predictions,dec_hidden,_ = decoder(dec_input,dec_hidden,enc_output)\n",
    "            y_true = targ[:,t]\n",
    "            y_pred = predictions\n",
    "            dec_input = tf.expand_dims(y_true,1)\n",
    "            loss += loss_function(y_true,y_pred)\n",
    "    batch_loss = (loss/int(targ.shape[1]))\n",
    "    variebles = encoder.variables+decoder.variables\n",
    "    gradients = tape.gradient(loss,variebles)\n",
    "    optimizer.apply_gradients(zip(gradients,variebles))\n",
    "    return batch_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.497164249420166\n",
      "Epoch 1 Batch 100 Loss 1.5411759614944458\n",
      "Epoch 1 Batch 200 Loss 1.499068021774292\n",
      "Epoch 1 Batch 300 Loss 1.3822156190872192\n",
      "Epoch 1 Loss 1.4581\n",
      "Time taken for 1 epoch 348.3586993217468 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0842732191085815\n",
      "Epoch 2 Batch 100 Loss 1.1192883253097534\n",
      "Epoch 2 Batch 200 Loss 1.1374292373657227\n",
      "Epoch 2 Batch 300 Loss 0.827667772769928\n",
      "Epoch 2 Loss 1.0672\n",
      "Time taken for 1 epoch 340.27195405960083 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.8148168921470642\n",
      "Epoch 3 Batch 100 Loss 0.8648380041122437\n",
      "Epoch 3 Batch 200 Loss 0.6795008778572083\n",
      "Epoch 3 Batch 300 Loss 0.6758356690406799\n",
      "Epoch 3 Loss 0.7507\n",
      "Time taken for 1 epoch 340.4019286632538 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.50692218542099\n",
      "Epoch 4 Batch 100 Loss 0.5582032203674316\n",
      "Epoch 4 Batch 200 Loss 0.5427631139755249\n",
      "Epoch 4 Batch 300 Loss 0.5362839698791504\n",
      "Epoch 4 Loss 0.5019\n",
      "Time taken for 1 epoch 379.56403136253357 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.2816670536994934\n",
      "Epoch 5 Batch 100 Loss 0.34010741114616394\n",
      "Epoch 5 Batch 200 Loss 0.36619147658348083\n",
      "Epoch 5 Batch 300 Loss 0.36841917037963867\n",
      "Epoch 5 Loss 0.3275\n",
      "Time taken for 1 epoch 399.32328486442566 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.16810010373592377\n",
      "Epoch 6 Batch 100 Loss 0.20189423859119415\n",
      "Epoch 6 Batch 200 Loss 0.25061333179473877\n",
      "Epoch 6 Batch 300 Loss 0.20018909871578217\n",
      "Epoch 6 Loss 0.2119\n",
      "Time taken for 1 epoch 399.1460418701172 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1667458564043045\n",
      "Epoch 7 Batch 100 Loss 0.15015363693237305\n",
      "Epoch 7 Batch 200 Loss 0.11133471131324768\n",
      "Epoch 7 Batch 300 Loss 0.13253247737884521\n",
      "Epoch 7 Loss 0.1414\n",
      "Time taken for 1 epoch 397.33945894241333 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.09630017727613449\n",
      "Epoch 8 Batch 100 Loss 0.08560024946928024\n",
      "Epoch 8 Batch 200 Loss 0.07389792054891586\n",
      "Epoch 8 Batch 300 Loss 0.09027548134326935\n",
      "Epoch 8 Loss 0.0998\n",
      "Time taken for 1 epoch 398.9126024246216 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.11745785921812057\n",
      "Epoch 9 Batch 100 Loss 0.090595543384552\n",
      "Epoch 9 Batch 200 Loss 0.05865482985973358\n",
      "Epoch 9 Batch 300 Loss 0.08285895735025406\n",
      "Epoch 9 Loss 0.0788\n",
      "Time taken for 1 epoch 397.96186423301697 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.09839969128370285\n",
      "Epoch 10 Batch 100 Loss 0.07094209641218185\n",
      "Epoch 10 Batch 200 Loss 0.07374471426010132\n",
      "Epoch 10 Batch 300 Loss 0.08126206696033478\n",
      "Epoch 10 Loss 0.0690\n",
      "Time taken for 1 epoch 398.10705518722534 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch,(inp,targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp,targ,enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if not batch%100:\n",
    "             print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy()}')\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                  total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}